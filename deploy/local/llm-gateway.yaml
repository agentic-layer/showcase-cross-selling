kind: Namespace
apiVersion: v1
metadata:
  name: llm-gateway
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config-file
  namespace: llm-gateway
data:
  config.yaml: |
      model_list:
        - model_name: gemini/gemini-2.5-flash-lite
          litellm_params:
            model: gemini/gemini-2.5-flash-lite
            api_key: os.environ/GOOGLE_API_KEY
      guardrails:
        - guardrail_name: "custom-pre-guard"
          litellm_params:
            guardrail: custom_guardrail.myCustomGuardrail
            mode: "pre_call"
            default_on: true
        - guardrail_name: "custom-during-guard"
          litellm_params:
            guardrail: custom_guardrail.myCustomGuardrail  
            mode: "during_call"
            default_on: true
        - guardrail_name: "custom-post-guard"
          litellm_params:
            guardrail: custom_guardrail.myCustomGuardrail
            mode: "post_call"
            default_on: true
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: litellm
  namespace: llm-gateway
spec:
  selector:
    matchLabels:
      app: litellm
  template:
    metadata:
      labels:
        app: litellm
    spec:
      containers:
      - name: litellm
        image: ghcr.io/berriai/litellm:v1.74.15-stable
        args:
          - "--config"
          - "/app/proxy_server_config.yaml"
        ports:
        - containerPort: 4000
        env:
        - name: GOOGLE_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-key-secrets
              key: GOOGLE_API_KEY
              optional: true
        - name: LITELLM_MASTER_KEY
          valueFrom:
            secretKeyRef:
              name: litellm-master-key-secrets
              key: LITELLM_MASTER_KEY
        - name: LITELLM_SALT_KEY
          value: "sk-salt"
        - name: DATABASE_URL
          value: "postgresql://postgres:my-secret-password@postgres:5432/litellm"
        - name: STORE_MODEL_IN_DB
          value: "TRUE"
        volumeMounts:
        - name: config-volume
          mountPath: /app/proxy_server_config.yaml
          subPath: config.yaml
        - name: guardrail-volume
          mountPath: /app/custom_guardrail.py
          subPath: custom_guardrail.py
      volumes:
      - name: config-volume
        configMap:
          name: litellm-config-file
      - name: guardrail-volume
        configMap:
          name: litellm-custom-guardrail
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-custom-guardrail
  namespace: llm-gateway
data:
  custom_guardrail.py: |
    from typing import Any, Dict, List, Literal, Optional, Union, AsyncGenerator

    import litellm
    from litellm._logging import verbose_proxy_logger
    from litellm.caching.caching import DualCache
    from litellm.integrations.custom_guardrail import CustomGuardrail
    from litellm.proxy._types import UserAPIKeyAuth
    from litellm.types.utils import ModelResponseStream
    
    
    class myCustomGuardrail(CustomGuardrail):
        def __init__(
            self,
            **kwargs,
        ):
            # store kwargs as optional_params
            self.optional_params = kwargs
    
            super().__init__(**kwargs)
    
        async def async_pre_call_hook(
            self,
            user_api_key_dict: UserAPIKeyAuth,
            cache: DualCache,
            data: dict,
            call_type: Literal[
                "completion",
                "text_completion",
                "embeddings",
                "image_generation",
                "moderation",
                "audio_transcription",
                "pass_through_endpoint",
                "rerank"
            ],
        ) -> Optional[Union[Exception, str, dict]]:
            """
            Runs before the LLM API call
            Runs on only Input
            Use this if you want to MODIFY the input
            """
    
            # In this guardrail, if a user inputs `litellm` we will mask it and then send it to the LLM
            _messages = data.get("messages")
            if _messages:
                for message in _messages:
                    _content = message.get("content")
                    if isinstance(_content, str):
                        if "litellm" in _content.lower():
                            _content = _content.replace("litellm", "another Gateway")
                            message["content"] = _content
            print(f"async_pre_call_hook: Message after masking ${_messages}")
            verbose_proxy_logger.info(
                "async_pre_call_hook: Message after masking %s", _messages
            )
    
            return data
    
        async def async_moderation_hook(
            self,
            data: dict,
            user_api_key_dict: UserAPIKeyAuth,
            call_type: Literal["completion", "embeddings", "image_generation", "moderation", "audio_transcription"],
        ):
            """
            Runs in parallel to LLM API call
            Runs on only Input
    
            This can NOT modify the input, only used to reject or accept a call before going to LLM API
            """
    
            # this works the same as async_pre_call_hook, but just runs in parallel as the LLM API Call
            # In this guardrail, if a user inputs `litellm` we will mask it.
            _messages = data.get("messages")
            if _messages:
                for message in _messages:
                    _content = message.get("content")
                    if isinstance(_content, str):
                        if "litellm" in _content.lower():
                            raise ValueError("Guardrail failed words - `litellm` detected")
    
        async def async_post_call_success_hook(
            self,
            data: dict,
            user_api_key_dict: UserAPIKeyAuth,
            response,
        ):
            """
            Runs on response from LLM API call
    
            It can be used to reject a response
    
            If a response contains the word "coffee" -> we will raise an exception
            """
            verbose_proxy_logger.debug("async_pre_call_hook response: %s", response)
            if isinstance(response, litellm.ModelResponse):
                for choice in response.choices:
                    if isinstance(choice, litellm.Choices):
                        verbose_proxy_logger.debug("async_pre_call_hook choice: %s", choice)
                        if (
                            choice.message.content
                            and isinstance(choice.message.content, str)
                            and "coffee" in choice.message.content
                        ):
                            raise ValueError("Guardrail failed Coffee Detected")
    
        async def async_post_call_streaming_iterator_hook(
            self,
            user_api_key_dict: UserAPIKeyAuth,
            response: Any,
            request_data: dict,
        ) -> AsyncGenerator[ModelResponseStream, None]:
            """
            Passes the entire stream to the guardrail
    
            This is useful for guardrails that need to see the entire response, such as PII masking.
    
            See Aim guardrail implementation for an example - https://github.com/BerriAI/litellm/blob/d0e022cfacb8e9ebc5409bb652059b6fd97b45c0/litellm/proxy/guardrails/guardrail_hooks/aim.py#L168
    
            Triggered by mode: 'post_call'
            """
            async for item in response:
                yield item
---
apiVersion: v1
kind: Service
metadata:
  name: litellm
  namespace: llm-gateway
  labels:
    app: litellm
spec:
  selector:
    app: litellm
  ports:
    - name: http
      port: 4000
      targetPort: 4000
      protocol: TCP
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: llm-gateway
  labels:
    app: postgres
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:17
        env:
        - name: POSTGRES_DB
          value: litellm
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_PASSWORD
          value: my-secret-password
        ports:
        - containerPort: 5432
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: llm-gateway
spec:
  selector:
    app: postgres
  ports:
  - name: http
    port: 5432
    targetPort: 5432
    protocol: TCP
  type: ClusterIP